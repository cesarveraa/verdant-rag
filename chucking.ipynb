{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas faiss-cpu tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wget https://gutenberg.org/cache/epub/14586/pg14586.txt -O the_brain.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./the_brain.txt\") as f:\n",
    "    the_brain = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "naive_chunks = text_splitter.split_text(the_brain)\n",
    "\n",
    "for chunk in naive_chunks[40:55]:\n",
    "    print(chunk + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-large\"), \n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "semantic_chunks = semantic_chunker.create_documents([the_brain])\n",
    "\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"MDT is associated with the basic\" in semantic_chunk.page_content:\n",
    "        print(semantic_chunk.page_content)\n",
    "        print(len(semantic_chunk.page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "semantic_chunk_vectorstore = FAISS.from_documents(\n",
    "    semantic_chunks, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    ")\n",
    "\n",
    "# Limitaremos semantic_chunk_vectorstore a k=1 para demostrar el poder de la estrategia de chunking semántico,\n",
    "# manteniendo un conteo de tokens similar entre el contexto recuperado semánticamente y el contexto recuperado de manera simple.\n",
    "\n",
    "semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "semantic_chunk_retriever.invoke(\"What is MDT?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Descargar el prompt del modelo RAG\n",
    "prompt = hub.pull(\"lm/rag-prompt\")\n",
    "\n",
    "# Generación\n",
    "# Utilizaremos ChatOpenAI para mantener la simplicidad del ejemplo\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# LCEL RAG Chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "semantic_rag_chain = {\n",
    "    \"context\": semantic_chunk_retriever, \n",
    "    \"question\": RunnablePassthrough()\n",
    "}\n",
    "\n",
    "# Definiendo el flujo de la cadena\n",
    "prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "semantic_rag_chain.invoke(\"What is MDT?\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
